# Eval run: `eval_20251212_053819Z`

## Config

```
{
  "eval": "/Users/ryanrodd/Desktop/Projects/ryan-prime/eval/basic.json",
  "models": [
    "gpt-4.1-2025-04-14-ryanprime-001"
  ],
  "system": null,
  "temperature": 0.7,
  "max_output_tokens": 400,
  "dry_run": true,
  "created_at_utc": "2025-12-12T05:38:19.358845+00:00",
  "conf_path": "/Users/ryanrodd/Desktop/Projects/ryan-prime/.conf"
}
```

## Counts

- **gpt-4.1-2025-04-14-ryanprime-001**: 30 prompts

## Notes

- `results.jsonl` contains one JSON object per prompt per model.
- For scoring, you can eyeball against `notes`, `good_signals`, and `red_flags`.

