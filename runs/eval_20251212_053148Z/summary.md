# Eval run: `eval_20251212_053148Z`

## Config

```
{
  "eval": "/Users/ryanrodd/Desktop/Projects/ryan-prime/eval/basic.json",
  "models": [
    "YOUR_FINE_TUNED_MODEL_ID",
    "gpt-4.1"
  ],
  "system": null,
  "temperature": 0.7,
  "max_output_tokens": 400,
  "dry_run": true,
  "created_at_utc": "2025-12-12T05:31:48.508072+00:00"
}
```

## Counts

- **YOUR_FINE_TUNED_MODEL_ID**: 30 prompts
- **gpt-4.1**: 30 prompts

## Notes

- `results.jsonl` contains one JSON object per prompt per model.
- For scoring, you can eyeball against `notes`, `good_signals`, and `red_flags`.

